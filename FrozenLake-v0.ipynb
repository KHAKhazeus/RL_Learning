{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n",
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def evaluation():\n",
    "    score = []\n",
    "    done = False\n",
    "    for i in range(10000):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = choose_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                score.append(reward)\n",
    "                break\n",
    "    \n",
    "    print(\"accuracy: {}\".format(np.mean(score)))\n",
    "# #initialize\n",
    "# qvalue = np.zeros([16,4])\n",
    "# evalue = np.zeros([16,4])\n",
    "# env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# #hyperparameters\n",
    "# alphaLearningRate = 0.3\n",
    "# faresightLambda = 0.95\n",
    "# decayRate = 0.1\n",
    "# decay_rate_of_epsilon = 0.001\n",
    "# max_epsilon = 1.0\n",
    "# min_epsilon = 0.00001 \n",
    "\n",
    "# #record score\n",
    "# score = []\n",
    "\n",
    "# #only greedy, no explore\n",
    "# def choose_action(state, epsilon):\n",
    "#     if random.random() >= epsilon:\n",
    "#         return np.argmax(qvalue[state])\n",
    "#     else:\n",
    "#         return random.randrange(4)\n",
    "\n",
    "# def SARSAcontrol():\n",
    "#     epsilon = 1.0\n",
    "#     for i in range(20000):\n",
    "# #         if i < 2000:\n",
    "# #             epsilon = random.uniform(0.9, 1.0)\n",
    "# #         elif i > 19000:\n",
    "# #             epsilon = 0\n",
    "# #         elif i % 3 == 0:\n",
    "# #             epsilon = random.random()\n",
    "# #         if i < 3000:\n",
    "# #             if i % 3 == 0:\n",
    "# #                 epsilon = 0\n",
    "# #             else:\n",
    "# #                 epsilon = random.uniform(0.8, 1.0)\n",
    "# #         elif i > 17000:\n",
    "# #             epsilon = random.uniform(0, 0.2)\n",
    "# #         else:\n",
    "# #             if i % 10 == 0:\n",
    "# #                 epsilon = random.random()\n",
    "# #             else:\n",
    "# #                 epsilon = 0\n",
    "#         previousState = env.reset()\n",
    "# #         epsilon = random.random()\n",
    "#         previousAction = choose_action(previousState, epsilon)\n",
    "#         evalue = np.zeros([16,4])\n",
    "#         score = []\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             steps = 0\n",
    "#             currentState, reward, done, info = env.step(previousAction)\n",
    "#             steps += 1\n",
    "#             currentAction = choose_action(currentState, epsilon)\n",
    "# #             if not reward and not done:\n",
    "# #                 reward = 0.05\n",
    "# #             elif (reward == 1 and done):\n",
    "# #                 reward = 100\n",
    "# #             elif (reward == 0 and done):\n",
    "# #                 reward = -1\n",
    "            \n",
    "#             error = reward + faresightLambda * qvalue[currentState][currentAction] \\\n",
    "#                 - qvalue[previousState][previousAction]\n",
    "# #             print(error)\n",
    "#             evalue[previousState][previousAction] += 1\n",
    "            \n",
    "#             nonZeroes = np.transpose(np.nonzero(evalue))\n",
    "#             for nonZeroe in nonZeroes:\n",
    "#                 row = nonZeroe[0]\n",
    "#                 column = nonZeroe[1]\n",
    "#                 qvalue[row][column] += alphaLearningRate * evalue[row][column] * error\n",
    "# #             qvalue[previousState][previousAction] += alphaLearningRate * error\n",
    "            \n",
    "#             evalue *= decayRate * faresightLambda\n",
    "            \n",
    "            \n",
    "#             previousState = currentState\n",
    "#             previousAction = currentAction\n",
    "#         epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate_of_epsilon*i)\n",
    "                \n",
    "#     print(\"training end\")\n",
    "#     print(qvalue)\n",
    "    \n",
    "# def evaluation():\n",
    "#     score = []\n",
    "#     done = False\n",
    "#     for i in range(10000):\n",
    "#         observation = env.reset()\n",
    "#         while True:\n",
    "#             action = choose_action(observation, 0)\n",
    "#             observation, reward, done, info = env.step(action)\n",
    "#             if done:\n",
    "#                 score.append(reward)\n",
    "#                 break\n",
    "    \n",
    "#     print(\"accuracy: {}\".format(np.mean(score)))\n",
    "\n",
    "\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196, changing this seems have no influence\n",
    ")\n",
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "def choose_action(observation):\n",
    "    return np.argmax(q_table[observation])\n",
    "\n",
    "alpha = 0.4\n",
    "gamma = 0.999\n",
    "\n",
    "q_table = dict([(x, [1, 1, 1, 1]) for x in range(16)])\n",
    "score = []\n",
    "\n",
    "def train():\n",
    "    for i in range(20000):\n",
    "        observation = env.reset()\n",
    "        action = choose_action(observation)\n",
    "\n",
    "        prev_observation = None\n",
    "        prev_action      = None\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        for t in range(2500):\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            action = choose_action(observation)\n",
    "\n",
    "            if not prev_observation is None:\n",
    "                q_old = q_table[prev_observation][prev_action]\n",
    "                q_new = q_old\n",
    "                if done:\n",
    "                    q_new += alpha * (reward - q_old)\n",
    "                else:\n",
    "                    q_new += alpha * (reward + gamma * q_table[observation][action] - q_old)\n",
    "\n",
    "                new_table = q_table[prev_observation]\n",
    "                new_table[prev_action] = q_new\n",
    "                q_table[prev_observation] = new_table\n",
    "\n",
    "            prev_observation = observation\n",
    "            prev_action = action\n",
    "\n",
    "            if done:\n",
    "                if len(score) < 100:\n",
    "                    score.append(reward)\n",
    "                else:\n",
    "                    score[i % 100] = reward\n",
    "\n",
    "    #             print(\"Episode {} finished after {} timesteps with r={}. Running score: {}\".format(i, t, reward, np.mean(score)))\n",
    "                break\n",
    "\n",
    "for batch in range(10):\n",
    "    train()\n",
    "    evaluation()\n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in range(10):\n",
    "#     SARSAcontrol()\n",
    "#     evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
