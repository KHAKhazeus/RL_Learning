{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#initialize\n",
    "qvalue = np.zeros([16,4])\n",
    "evalue = np.zeros([16,4])\n",
    "\n",
    "#slippery\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "#non-slippery\n",
    "# from gym.envs.registration import register\n",
    "# register(\n",
    "#     id='FrozenLakeNotSlippery-v0',\n",
    "#     entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "#     kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "#     max_episode_steps=100,\n",
    "#     reward_threshold=0.8196, # optimum = .8196, changing this seems have no influence\n",
    "# )\n",
    "# env = gym.make(\"FrozenLakeNotSlippery-v0\")\n",
    "\n",
    "#hyperparameters\n",
    "alphaLearningRate = 0.3\n",
    "faresightLambda = 0.95\n",
    "decayRate = 0.0\n",
    "decay_rate_of_epsilon = 0.001\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.00001 \n",
    "\n",
    "#record score\n",
    "score = []\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.random() >= epsilon:\n",
    "        return np.argmax(qvalue[state])\n",
    "    else:\n",
    "        return random.randrange(4)\n",
    "\n",
    "def SARSAcontrol():\n",
    "    epsilon = 1.0\n",
    "    for i in range(20000):\n",
    "        previousState = env.reset()\n",
    "        previousAction = choose_action(previousState, epsilon)\n",
    "        evalue = np.zeros([16,4])\n",
    "        score = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            steps = 0\n",
    "            currentState, reward, done, info = env.step(previousAction)\n",
    "            steps += 1\n",
    "            currentAction = choose_action(currentState, epsilon)\n",
    "            error = reward + faresightLambda * qvalue[currentState][currentAction] \\\n",
    "                - qvalue[previousState][previousAction]\n",
    "            evalue[previousState][previousAction] += 1\n",
    "            \n",
    "            nonZeroes = np.transpose(np.nonzero(evalue))\n",
    "            for nonZeroe in nonZeroes:\n",
    "                row = nonZeroe[0]\n",
    "                column = nonZeroe[1]\n",
    "                qvalue[row][column] += alphaLearningRate * evalue[row][column] * error\n",
    "            \n",
    "            evalue *= decayRate * faresightLambda\n",
    "            \n",
    "            \n",
    "            previousState = currentState\n",
    "            previousAction = currentAction\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate_of_epsilon*i)\n",
    "                \n",
    "    print(\"training end\")\n",
    "    print(qvalue)\n",
    "    \n",
    "def evaluation():\n",
    "    score = []\n",
    "    done = False\n",
    "    for i in range(10000):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = choose_action(observation, 0)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                score.append(reward)\n",
    "                break\n",
    "    \n",
    "    print(\"accuracy: {}\".format(np.mean(score)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
